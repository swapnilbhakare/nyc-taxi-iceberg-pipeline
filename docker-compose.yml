version: "3.8"

services:
  # Step 1: PostgreSQL - Backend database for Hive Metastore
  postgres:
    image: postgres:11
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Step 2: Hive Metastore - Catalog service (waits for PostgreSQL)
  hive-metastore:
    image: apache/hive:3.1.3
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SKIP_SCHEMA_INIT: "false"
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive -Dhive.metastore.warehouse.dir=/opt/hive/data/warehouse -Dhive.metastore.uris=thrift://0.0.0.0:9083"
    ports:
      - "9083:9083"
    volumes:
      - warehouse:/opt/hive/data/warehouse

  # Base image with common dependencies
  base:
    build:
      context: .
      dockerfile: Dockerfile.base
    image: nyc-taxi-base:latest

  # Step 3: ETL Job - PySpark processing (waits for HMS, runs once)
  etl:
    build:
      context: .
      dockerfile: Dockerfile.etl
    depends_on:
      hive-metastore:
        condition: service_started
    volumes:
      - warehouse:/opt/hive/data/warehouse
    command: >
      bash -c "
        sleep 30
        python etl.py
      "

  # Step 4: FastAPI - REST service (waits for ETL to complete)
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    depends_on:
      etl:
        condition: service_completed_successfully
    ports:
      - "8000:8000"
    volumes:
      - warehouse:/opt/hive/data/warehouse

volumes:
  postgres_data:
  warehouse:
